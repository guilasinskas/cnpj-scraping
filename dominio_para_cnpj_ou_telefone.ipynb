{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8372,"status":"ok","timestamp":1689340145916,"user":{"displayName":"Davi Braga Moreira","userId":"03445479500401572171"},"user_tz":180},"id":"69seBt35kZWW","outputId":"185f29f3-aaee-44cd-e57c-4db8c4c52b80"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting requests_html\n","  Downloading requests_html-0.10.0-py3-none-any.whl (13 kB)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from requests_html) (2.27.1)\n","Collecting pyquery (from requests_html)\n","  Downloading pyquery-2.0.0-py3-none-any.whl (22 kB)\n","Collecting fake-useragent (from requests_html)\n","  Downloading fake_useragent-1.1.3-py3-none-any.whl (50 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/50.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.5/50.5 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting parse (from requests_html)\n","  Downloading parse-1.19.1-py2.py3-none-any.whl (18 kB)\n","Collecting bs4 (from requests_html)\n","  Downloading bs4-0.0.1.tar.gz (1.1 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting w3lib (from requests_html)\n","  Downloading w3lib-2.1.1-py3-none-any.whl (21 kB)\n","Collecting pyppeteer>=0.0.14 (from requests_html)\n","  Downloading pyppeteer-1.0.2-py3-none-any.whl (83 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.4/83.4 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: appdirs<2.0.0,>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from pyppeteer>=0.0.14->requests_html) (1.4.4)\n","Requirement already satisfied: certifi>=2021 in /usr/local/lib/python3.10/dist-packages (from pyppeteer>=0.0.14->requests_html) (2023.5.7)\n","Collecting importlib-metadata>=1.4 (from pyppeteer>=0.0.14->requests_html)\n","  Downloading importlib_metadata-6.8.0-py3-none-any.whl (22 kB)\n","Collecting pyee<9.0.0,>=8.1.0 (from pyppeteer>=0.0.14->requests_html)\n","  Downloading pyee-8.2.2-py2.py3-none-any.whl (12 kB)\n","Requirement already satisfied: tqdm<5.0.0,>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from pyppeteer>=0.0.14->requests_html) (4.65.0)\n","Requirement already satisfied: urllib3<2.0.0,>=1.25.8 in /usr/local/lib/python3.10/dist-packages (from pyppeteer>=0.0.14->requests_html) (1.26.16)\n","Collecting websockets<11.0,>=10.0 (from pyppeteer>=0.0.14->requests_html)\n","  Downloading websockets-10.4-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (106 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.8/106.8 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from bs4->requests_html) (4.11.2)\n","Requirement already satisfied: lxml>=2.1 in /usr/local/lib/python3.10/dist-packages (from pyquery->requests_html) (4.9.3)\n","Collecting cssselect>=1.2.0 (from pyquery->requests_html)\n","  Downloading cssselect-1.2.0-py2.py3-none-any.whl (18 kB)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->requests_html) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->requests_html) (3.4)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=1.4->pyppeteer>=0.0.14->requests_html) (3.16.0)\n","Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->bs4->requests_html) (2.4.1)\n","Building wheels for collected packages: bs4\n","  Building wheel for bs4 (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for bs4: filename=bs4-0.0.1-py3-none-any.whl size=1257 sha256=8d4c616bac303bcda93872f5b8b32049b878b1290ce4bb39f1164867e1e49e7a\n","  Stored in directory: /root/.cache/pip/wheels/25/42/45/b773edc52acb16cd2db4cf1a0b47117e2f69bb4eb300ed0e70\n","Successfully built bs4\n","Installing collected packages: pyee, parse, fake-useragent, websockets, w3lib, importlib-metadata, cssselect, pyquery, pyppeteer, bs4, requests_html\n","Successfully installed bs4-0.0.1 cssselect-1.2.0 fake-useragent-1.1.3 importlib-metadata-6.8.0 parse-1.19.1 pyee-8.2.2 pyppeteer-1.0.2 pyquery-2.0.0 requests_html-0.10.0 w3lib-2.1.1 websockets-10.4\n"]}],"source":["%pip install requests_html;"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15779,"status":"ok","timestamp":1689343537536,"user":{"displayName":"Davi Braga Moreira","userId":"03445479500401572171"},"user_tz":180},"id":"1vW56gvCkbkd","outputId":"53fd3812-d2cb-4575-8649-6c02bcc05a34"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n"]}],"source":["import requests\n","import urllib\n","import re\n","import time\n","import pandas as pd\n","from requests_html import HTML\n","from requests_html import HTMLSession\n","\n","src_df = pd.read_csv('cnpj.csv')\n","\n","def get_source(url):\n","\n","    try:\n","        session = HTMLSession()\n","        response = session.get(url)\n","        return response\n","\n","    except requests.exceptions.RequestException as e:\n","        print(e)\n","\n","def scrape_google(query):\n","\n","    query = urllib.parse.quote_plus(query)\n","    response = get_source(\"https://www.google.com.br/search?q=\" + query)\n","\n","    links = list(response.html.absolute_links)\n","    google_domains = ('https://www.google.',\n","                      'https://google.',\n","                      'https://webcache.googleusercontent.',\n","                      'http://webcache.googleusercontent.',\n","                      'https://policies.google.',\n","                      'https://support.google.',\n","                      'https://maps.google.')\n","\n","    for url in links[:]:\n","        if url.startswith(google_domains):\n","            links.remove(url)\n","\n","    return links\n","\n","def get_results(query):\n","\n","    query = urllib.parse.quote_plus(query)\n","    response = get_source(\"https://www.google.com.br/search?q=\" + query)\n","\n","    return response\n","\n","def parse_results(response):\n","\n","    css_identifier_result = \".tF2Cxc\"\n","    css_identifier_title = \"h3\"\n","    css_identifier_link = \".yuRUbf a\"\n","    css_identifier_text = \".VwiC3b\"\n","\n","    results = response.html.find(css_identifier_result)\n","\n","    output = []\n","\n","    for result in results:\n","\n","        item = {\n","            'title': result.find(css_identifier_title, first=True).text,\n","            'link': result.find(css_identifier_link, first=True).attrs['href'],\n","            'text': result.find(css_identifier_text, first=True).text\n","        }\n","\n","        output.append(item)\n","\n","    return output\n","\n","def google_search(query):\n","    response = get_results(query)\n","    return parse_results(response)\n","\n","def get_cnpj(soup):\n","\n","    phones = []\n","    phone = re.findall(r\"([0-9]{2}[\\.]?[0-9]{3}[\\.]?[0-9]{3}[\\/]?[0-9]{4}[-]?[0-9]{2})\", soup)\n","    phones.extend(phone)\n","\n","    if(len(phones) == 0): return \"\"\n","    else: return phones[0]\n","\n","def get_phone(soup):\n","\n","    phones = []\n","    phone = re.findall(r'\\(\\d{2}\\)?\\s\\d{4,5}\\-\\d{4}', soup)\n","    phones.extend(phone)\n","\n","    phone = re.findall(r'\\(\\b\\(?\\d{2}\\)?\\s\\d{4,5}\\-\\d{4}', soup)\n","    phones.extend(phone)\n","\n","    phone = re.findall(r'\\(?\\d{2}\\)?\\s\\d{4,5}\\-\\d{4}', soup)\n","    phones.extend(phone)\n","\n","    phone = re.findall(r'\\(?\\d{2}\\)\\d{4,5}\\-\\d{4}', soup)\n","    phones.extend(phone)\n","\n","    if(len(phones) == 0): return \"\"\n","    else: return phones[0]\n","\n","def formata(telefone: str) -> str:\n","    try:\n","        NUMEROS = \"0123456789\"\n","        numeros = ''\n","        for digito in telefone:\n","            if (digito in NUMEROS):\n","                numeros += digito\n","\n","        mask = \"\"\n","        if (len(numeros) == 14):\n","            mask = \"%s%s.%s%s%s.%s%s%s/%s%s%s%s-%s%s\"\n","\n","        mask = mask % tuple(numeros)\n","        return mask\n","    except:\n","        return \"\"\n","\n","\n","for i, row in src_df.iterrows():\n","    url = str(row['Dominios']) + \"cnpj\"\n","    try:\n","        results = str(google_search(url))\n","    except:\n","        print ('Unsucessful: ' + str(url))\n","        continue\n","\n","    cnpj = get_cnpj(results)\n","    print(cnpj)\n","\n","\n","    src_df.loc[i,'CNPJ'] = cnpj\n","    time.sleep(0)\n","\n","src_df[\"CNPJ\"] = [formata(x) for x in src_df[\"CNPJ\"]]\n","src_df.to_excel('output.xlsx', index=False)"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyNWjuli6YdHr5Eb+LhwTxlj","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
